{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f167aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL part 1 \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_customers = pd.read_csv(\"/Users/thomassimmons/c/new/data/customers.csv\")\n",
    "df_oi = pd.read_csv(\"/Users/thomassimmons/c/new/data/order_items.csv\")\n",
    "df_orders = pd.read_csv(\"/Users/thomassimmons/c/new/data/orders.csv\")\n",
    "df_products = pd.read_csv(\"/Users/thomassimmons/c/new/data/products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c1f1ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers: \n",
      "    customer_id customer_name                       email           phone  \\\n",
      "0         5001  Jordan Davis  Jordan.Davis14@example.com      4893252880   \n",
      "1         5002   Casey Brown   CASEY.BROWN31@EXAMPLE.COM    911-718-2278   \n",
      "2         5003  Taylor Davis  Taylor.Davis97@example.com  (346) 578-7133   \n",
      "3         5004   Drew Garcia    DREW.GARCIA1@EXAMPLE.COM    031-051-8347   \n",
      "4         5005   Drew Wilson   DREW.WILSON61@EXAMPLE.COM  (763) 116-5667   \n",
      "\n",
      "      city  \n",
      "0   boston  \n",
      "1    Miami  \n",
      "2   Austin  \n",
      "3  Seattle  \n",
      "4  seattle  \n"
     ]
    }
   ],
   "source": [
    "# Getting a quick look at each\n",
    "print(\"Customers: \\n\", df_customers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f0bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order information: \n",
      "    order_id  product_id  quantity unit_price\n",
      "0     20001        1008       7.0    $288.34\n",
      "1     20001        1002       2.0    $419.29\n",
      "2     20001        1002       4.0     $95.60\n",
      "3     20002        9999       5.0    $464.44\n",
      "4     20002        1013       5.0    $444.96\n"
     ]
    }
   ],
   "source": [
    "print(\"Order items: \\n\", df_oi.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "316fed83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders:     order_id  customer_id  order_date     status order_total\n",
      "0     20001         5012  2025-01-17       PAID   $1,461.70\n",
      "1     20002         5009  2025/04/13        NEW     $709.40\n",
      "2     20003         5012  2025-07-06  CANCELLED     $107.36\n",
      "3     20004         5007  2025-04-11    shipped     $379.80\n",
      "4     20005         5027  04/05/2025  CANCELLED     $669.65\n"
     ]
    }
   ],
   "source": [
    "print(\"Orders: \", df_orders.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afa30ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers:\n",
      "    customer_id customer_name                       email           phone  \\\n",
      "0         5001  Jordan Davis  Jordan.Davis14@example.com      4893252880   \n",
      "1         5002   Casey Brown   CASEY.BROWN31@EXAMPLE.COM    911-718-2278   \n",
      "2         5003  Taylor Davis  Taylor.Davis97@example.com  (346) 578-7133   \n",
      "3         5004   Drew Garcia    DREW.GARCIA1@EXAMPLE.COM    031-051-8347   \n",
      "4         5005   Drew Wilson   DREW.WILSON61@EXAMPLE.COM  (763) 116-5667   \n",
      "\n",
      "      city  \n",
      "0   boston  \n",
      "1    Miami  \n",
      "2   Austin  \n",
      "3  Seattle  \n",
      "4  seattle  \n"
     ]
    }
   ],
   "source": [
    "print(\"Customers:\\n\", df_customers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1937d8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_id      0\n",
       "product_id    0\n",
       "quantity      0\n",
       "unit_price    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customers.isnull().sum()\n",
    "df_oi.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bea457cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oi.dropna(inplace=True)\n",
    "df_products.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdadcc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id      0\n",
       "product_name    0\n",
       "category        0\n",
       "list_price      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customers.isnull().sum()\n",
    "df_products.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d70d7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-12 19:09:20] Starting ETL…\n",
      "ETL complete. SQLite database at: /Users/thomassimmons/c/new/data/retail.db\n",
      "customers -> rows: 35 cols: 5\n",
      "products -> rows: 17 cols: 4\n",
      "orders -> rows: 17 cols: 4\n",
      "order_items -> rows: 455 cols: 5\n"
     ]
    }
   ],
   "source": [
    "# etl.py\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "DATA_DIR = \"/Users/thomassimmons/c/new/data\"\n",
    "FILES = {\n",
    "    \"products\": os.path.join(DATA_DIR, \"/Users/thomassimmons/c/new/data/products.csv\"),\n",
    "    \"orders\": os.path.join(DATA_DIR, \"/Users/thomassimmons/c/new/data/products.csv\"),\n",
    "    \"order_items\": os.path.join(DATA_DIR, \"/Users/thomassimmons/c/new/data/order_items.csv\"),\n",
    "    \"customers\": os.path.join(DATA_DIR, \"/Users/thomassimmons/c/new/data/customers.csv\"),\n",
    "}\n",
    "DB_PATH = os.path.join(DATA_DIR, \"retail.db\")\n",
    "IF_EXISTS = \"replace\"  # 'fail' | 'replace' | 'append'\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def snake(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"[^\\w]+\", \"_\", s)\n",
    "    s = re.sub(r\"__+\", \"_\", s)\n",
    "    return s.strip(\"_\").lower()\n",
    "\n",
    "def clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [snake(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def trim_strings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_string_dtype(df[c]):\n",
    "            df[c] = df[c].astype(\"string\").str.strip()\n",
    "    return df\n",
    "\n",
    "def coerce_numeric(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def coerce_dates(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\", utc=False)\n",
    "    return df\n",
    "\n",
    "def dedupe(df: pd.DataFrame, key_cols: list[str] | None = None) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if key_cols and all(k in df.columns for k in key_cols):\n",
    "        return df.drop_duplicates(subset=key_cols, keep=\"first\").reset_index(drop=True)\n",
    "    return df.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "def read_csv_flex(path: str) -> pd.DataFrame:\n",
    "    # Try sensible defaults. If you know your separators/encodings, set them explicitly.\n",
    "    return pd.read_csv(path, low_memory=False)\n",
    "\n",
    "def write_df(con, name: str, df: pd.DataFrame):\n",
    "    df.to_sql(name, con, if_exists=IF_EXISTS, index=False)\n",
    "\n",
    "def add_indexes(con, table: str, indexes: list[tuple[str, str]]):\n",
    "    # indexes: [(index_name, column_expression)]\n",
    "    cur = con.cursor()\n",
    "    for idx_name, col_expr in indexes:\n",
    "        cur.execute(f'DROP INDEX IF EXISTS \"{idx_name}\"')\n",
    "        cur.execute(f'CREATE INDEX \"{idx_name}\" ON \"{table}\" ({col_expr})')\n",
    "    con.commit()\n",
    "\n",
    "# ---------- EXTRACT ----------\n",
    "def extract() -> dict[str, pd.DataFrame]:\n",
    "    dfs = {}\n",
    "    for name, path in FILES.items():\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "        df = read_csv_flex(path)\n",
    "        df = clean_columns(df)\n",
    "        df = trim_strings(df)\n",
    "        dfs[name] = df\n",
    "    return dfs\n",
    "\n",
    "# ---------- TRANSFORM ----------\n",
    "def transform(dfs: dict[str, pd.DataFrame]) -> dict[str, pd.DataFrame]:\n",
    "    # Guess common numeric/date fields and coerce if present\n",
    "    numeric_candidates = {\n",
    "        \"products\": [\"price\", \"cost\", \"msrp\"],\n",
    "        \"orders\": [\"total_amount\", \"subtotal\", \"tax\", \"shipping_cost\", \"discount\"],\n",
    "        \"order_items\": [\"quantity\", \"unit_price\", \"line_total\", \"discount\"],\n",
    "        \"customers\": [],\n",
    "    }\n",
    "    date_candidates = {\n",
    "        \"orders\": [\"order_date\", \"ship_date\", \"delivery_date\", \"created_at\", \"updated_at\"],\n",
    "        \"customers\": [\"created_at\", \"updated_at\", \"signup_date\"],\n",
    "        \"products\": [\"created_at\", \"updated_at\", \"release_date\"],\n",
    "        \"order_items\": [\"created_at\", \"updated_at\"],\n",
    "    }\n",
    "\n",
    "    # Apply coercions + dedupe\n",
    "    for name, df in dfs.items():\n",
    "        df = coerce_numeric(df, numeric_candidates.get(name, []))\n",
    "        df = coerce_dates(df, date_candidates.get(name, []))\n",
    "        # Deduplicate on common keys when obvious\n",
    "        if name == \"customers\":\n",
    "            key = [c for c in [\"customer_id\", \"id\"] if c in df.columns][:1]\n",
    "            df = dedupe(df, key_cols=key or None)\n",
    "        elif name == \"products\":\n",
    "            key = [c for c in [\"product_id\", \"id\"] if c in df.columns][:1]\n",
    "            df = dedupe(df, key_cols=key or None)\n",
    "        elif name == \"orders\":\n",
    "            key = [c for c in [\"order_id\", \"id\"] if c in df.columns][:1]\n",
    "            df = dedupe(df, key_cols=key or None)\n",
    "        elif name == \"order_items\":\n",
    "            # Often order_items has a surrogate key or is unique on (order_id, product_id, maybe line_number)\n",
    "            key = [k for k in [\"order_item_id\", \"id\"] if k in df.columns][:1]\n",
    "            sub = [k for k in [\"order_id\", \"product_id\", \"line_number\"] if k in df.columns]\n",
    "            df = dedupe(df, key_cols=key or (sub if len(sub) >= 2 else None))\n",
    "        dfs[name] = df\n",
    "\n",
    "    # Optional: derive clean totals if possible\n",
    "    if \"order_items\" in dfs:\n",
    "        oi = dfs[\"order_items\"].copy()\n",
    "        if \"line_total\" not in oi.columns:\n",
    "            if {\"quantity\", \"unit_price\"}.issubset(oi.columns):\n",
    "                oi[\"line_total\"] = (oi[\"quantity\"].astype(\"float\") * oi[\"unit_price\"].astype(\"float\"))\n",
    "        dfs[\"order_items\"] = oi\n",
    "\n",
    "    # Optional: build a star-schema fact table if columns exist\n",
    "    can_build_fact = all(k in dfs for k in [\"customers\", \"orders\", \"order_items\", \"products\"])\n",
    "    fact = None\n",
    "    if can_build_fact:\n",
    "        cust = dfs[\"customers\"].copy()\n",
    "        ords = dfs[\"orders\"].copy()\n",
    "        items = dfs[\"order_items\"].copy()\n",
    "        prods = dfs[\"products\"].copy()\n",
    "\n",
    "        # Key discovery\n",
    "        cid = \"customer_id\" if \"customer_id\" in ords.columns else (\"customer_id\" if \"customer_id\" in cust.columns else None)\n",
    "        oid = \"order_id\" if \"order_id\" in items.columns else (\"order_id\" if \"order_id\" in ords.columns else None)\n",
    "        pid = \"product_id\" if \"product_id\" in items.columns else (\"product_id\" if \"product_id\" in prods.columns else None)\n",
    "        order_date_col = \"order_date\" if \"order_date\" in ords.columns else None\n",
    "\n",
    "        if all([cid, oid, pid]) and cid in ords.columns and oid in ords.columns and pid in items.columns:\n",
    "            # Join: items -> orders\n",
    "            fact = items.merge(ords, on=oid, how=\"left\", suffixes=(\"\", \"_order\"))\n",
    "            # Join: + customers\n",
    "            if cid in fact.columns and cid in cust.columns:\n",
    "                fact = fact.merge(cust, on=cid, how=\"left\", suffixes=(\"\", \"_customer\"))\n",
    "            # Join: + products\n",
    "            if pid in fact.columns and pid in prods.columns:\n",
    "                fact = fact.merge(prods, on=pid, how=\"left\", suffixes=(\"\", \"_product\"))\n",
    "\n",
    "            # Keep handy analytics fields if present\n",
    "            keep_dates = [c for c in [order_date_col] if c and c in fact.columns]\n",
    "            keep_nums = [c for c in [\"quantity\", \"unit_price\", \"line_total\", \"total_amount\"] if c in fact.columns]\n",
    "            id_cols = [c for c in [oid, cid, pid, \"order_item_id\"] if c in fact.columns]\n",
    "            # plus a reasonable set of descriptors if they exist\n",
    "            maybe_desc = [c for c in [\"status\", \"state\", \"city\", \"category\", \"sub_category\", \"brand\"] if c in fact.columns]\n",
    "            base_cols = list(dict.fromkeys(id_cols + keep_dates + keep_nums + maybe_desc))\n",
    "            fact = fact.loc[:, base_cols].copy()\n",
    "\n",
    "            # Derive totals if still missing\n",
    "            if \"line_total\" not in fact.columns and {\"quantity\", \"unit_price\"}.issubset(fact.columns):\n",
    "                fact[\"line_total\"] = fact[\"quantity\"].astype(float) * fact[\"unit_price\"].astype(float)\n",
    "\n",
    "            # Add simple date parts\n",
    "            if order_date_col and pd.api.types.is_datetime64_any_dtype(fact[order_date_col]):\n",
    "                fact[\"order_year\"] = fact[order_date_col].dt.year\n",
    "                fact[\"order_month\"] = fact[order_date_col].dt.month\n",
    "                fact[\"order_day\"] = fact[order_date_col].dt.day\n",
    "\n",
    "            dfs[\"fact_order_items\"] = fact\n",
    "\n",
    "    return dfs\n",
    "\n",
    "# ---------- LOAD ----------\n",
    "def load(dfs: dict[str, pd.DataFrame], db_path: str = DB_PATH):\n",
    "    if os.path.exists(db_path):\n",
    "        os.remove(db_path) if IF_EXISTS == \"replace\" else None\n",
    "    con = sqlite3.connect(db_path)\n",
    "\n",
    "    # Write base tables\n",
    "    for name, df in dfs.items():\n",
    "        write_df(con, name, df)\n",
    "\n",
    "    # Add useful indexes if columns exist\n",
    "    if \"orders\" in dfs:\n",
    "        cols = dfs[\"orders\"].columns\n",
    "        idx = []\n",
    "        if \"order_id\" in cols: idx.append((\"idx_orders_order_id\", \"order_id\"))\n",
    "        if \"customer_id\" in cols: idx.append((\"idx_orders_customer_id\", \"customer_id\"))\n",
    "        if \"order_date\" in cols: idx.append((\"idx_orders_order_date\", \"order_date\"))\n",
    "        add_indexes(con, \"orders\", idx)\n",
    "\n",
    "    if \"order_items\" in dfs:\n",
    "        cols = dfs[\"order_items\"].columns\n",
    "        idx = []\n",
    "        if \"order_id\" in cols: idx.append((\"idx_order_items_order_id\", \"order_id\"))\n",
    "        if \"product_id\" in cols: idx.append((\"idx_order_items_product_id\", \"product_id\"))\n",
    "        add_indexes(con, \"order_items\", idx)\n",
    "\n",
    "    if \"fact_order_items\" in dfs:\n",
    "        cols = dfs[\"fact_order_items\"].columns\n",
    "        idx = []\n",
    "        if \"order_id\" in cols: idx.append((\"idx_fact_order_id\", \"order_id\"))\n",
    "        if \"customer_id\" in cols: idx.append((\"idx_fact_customer_id\", \"customer_id\"))\n",
    "        if \"product_id\" in cols: idx.append((\"idx_fact_product_id\", \"product_id\"))\n",
    "        if \"order_year\" in cols: idx.append((\"idx_fact_year\", \"order_year\"))\n",
    "        if \"order_month\" in cols: idx.append((\"idx_fact_month\", \"order_month\"))\n",
    "        add_indexes(con, \"fact_order_items\", idx)\n",
    "\n",
    "    con.close()\n",
    "    return db_path\n",
    "\n",
    "# ---------- RUN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Starting ETL…\")\n",
    "    dfs = extract()\n",
    "    dfs = transform(dfs)\n",
    "    db_file = load(dfs, DB_PATH)\n",
    "    print(f\"ETL complete. SQLite database at: {db_file}\")\n",
    "\n",
    "    # Quick sanity prints\n",
    "    for t in [\"customers\", \"products\", \"orders\", \"order_items\", \"fact_order_items\"]:\n",
    "        if t in dfs:\n",
    "            print(t, \"-> rows:\", len(dfs[t]), \"cols:\", len(dfs[t].columns))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
